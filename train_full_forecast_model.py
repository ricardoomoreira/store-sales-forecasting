import pickle
import random

import torch
import torch.nn as nn
import torch.optim as optim

########################################################################################
# DESCRIPTION
########################################################################################
#
# If you are reading the Full Forecast Model first, I recommend you first check the
# description of Sequential Model first (train_sequential_model.py) since this one is an
# iteration over that sequential model.

# Model Input
# -----------
# The full forecast model inputs are the same as the Sequential Model.


# Model Architecture
# ------------------
# The full forecast model uses the same as the Sequential Model.

# Full Forecast Model
# -------------------
# The goal of the Kaggle exercise was to predict for all the 1782 store-product a 16-day
# worth of sales forecast.
# The main problem with this is that a typical sequential model will start predicting
# for day 1 informed with the historical data, but then will use it's own perdictions
# to inform the upcomming days from day 2 to 16. And as the model moves over the 16 days,
# the errors acumulate.
# To avoid this problem, one solution would be to train the model to be independent of
# the previous "sales" values. My bet is that this will make the model much weaker
# since it will not learn from the "sales" signal it is trying to predict.
# An alternative solution was to make the model train to predict a full forecast of 16
# days ahead for each day in the training set. This will make the model ready for making
# a single prediction in the test set using only the training historical data.
# Despite being more challeging in training, this will create a model that has no error
# propagation when predicting for this specific Kaggle exercise which only requires a
# 16 day forecast.
# This model uses the same principle as the Sequential Model in which it will use the
# previous "sales" values for the past 16 days and will combine each of these values in
# 16 different ways to generate 16 future "sales" values. In practice the model
# generates an output of 16x16 values which are then converted into a 16 by 16 matrix
# which is used to compute 16 dot products with the same y_prev (1 by 16) and results
# into a (1 by 16) forecast vector.

########################################################################################

# Load inputs (generated by running data_pre_processing.py)
with open("seqs.pkl", "rb") as f:
    (
        seqs,
        splits,
        FORECAST_WINDOW,
        COLS,
        STORE_TO_I,
        I_TO_STORE,
        FAM_TO_I,
        I_TO_FAM,
        PROMO_TO_I,
        I_TO_PROMO,
        MONTH_TO_I,
        I_TO_MONTH,
        CITY_TO_I,
        I_TO_CITY,
        STATE_TO_I,
        I_TO_STATE,
        TYPE_TO_I,
        I_TO_TYPE,
        CLUSTER_TO_I,
        I_TO_CLUSTER,
    ) = pickle.load(f)


# Define model architecture.
class ForecastModel(nn.Module):
    def __init__(self, n_hidden, n_output, emb_sz=2, num_layers=2, batch_first=True):
        super(ForecastModel, self).__init__()
        self.embed_store = nn.Embedding(len(STORE_TO_I), emb_sz)
        self.embed_family = nn.Embedding(len(FAM_TO_I), emb_sz)
        self.embed_city = nn.Embedding(len(CITY_TO_I), emb_sz)
        self.embed_state = nn.Embedding(len(STATE_TO_I), emb_sz)
        self.embed_type = nn.Embedding(len(TYPE_TO_I), emb_sz)
        self.embed_cluster = nn.Embedding(len(CLUSTER_TO_I), emb_sz)
        self.embed_promo = nn.Embedding(len(PROMO_TO_I), emb_sz)
        self.embed_month = nn.Embedding(len(MONTH_TO_I), emb_sz)
        self.rnn = nn.LSTM(
            8 * emb_sz + 5, n_hidden, num_layers=num_layers, batch_first=batch_first
        )
        self.relu = nn.ReLU()
        self.out = nn.Linear(n_hidden, n_output)

    def get_col(self, x, col):
        if x.ndim == 1:
            return x[COLS.index(col)].long()
        elif x.ndim == 2:
            return x[:, COLS.index(col)].long()
        elif x.ndim == 3:
            return x[:, :, COLS.index(col)].long()

    def get_cols(self, x, cols):
        if x.ndim == 1:
            return x[[COLS.index(c) for c in cols]]
        elif x.ndim == 2:
            return x[:, [COLS.index(c) for c in cols]]
        elif x.ndim == 3:
            return x[:, :, [COLS.index(c) for c in cols]]

    def forward(self, x, h, c):
        store = self.embed_store(self.get_col(x, "store"))
        family = self.embed_family(self.get_col(x, "family"))
        city = self.embed_city(self.get_col(x, "city"))
        state = self.embed_state(self.get_col(x, "state"))
        xtype = self.embed_type(self.get_col(x, "type"))
        cluster = self.embed_cluster(self.get_col(x, "cluster"))
        promo = self.embed_promo(self.get_col(x, "is_promotion"))
        month = self.embed_month(self.get_col(x, "month"))
        x_num = self.get_cols(
            x,
            [
                "sales",
                "dist_to_payday",
                "dist_to_next_holiday",
                "weekday",
                "oil_price",
            ],
        )
        x_w_embs = torch.cat(
            [store, family, city, state, xtype, cluster, promo, month, x_num],
            dim=x.ndim - 1,
        ).float()
        out_h, (h, c) = self.rnn(x_w_embs, (h, c))
        out = self.relu(self.out(self.relu(out_h)))
        return out, h, c


def main():
    # Model parameters
    HIDDEN_SIZE = 100
    NUM_LAYERS = 2
    EMB_SZ = 2
    BATCH_SIZE = 16
    PATH = f"model_full_{HIDDEN_SIZE}_{NUM_LAYERS}_{EMB_SZ}_{BATCH_SIZE}.pth"

    # Initialize model
    model = ForecastModel(
        HIDDEN_SIZE,
        FORECAST_WINDOW * FORECAST_WINDOW,
        emb_sz=EMB_SZ,
        num_layers=NUM_LAYERS,
        batch_first=False,
    )
    model.float()

    # Set optimizer.
    optimizer = optim.Adam(model.parameters(), lr=10**-2.2)

    # Output is the attention weights for the previous 16 days of sales times the
    # number of forecasted outputs (also 16).
    # Check the formula of the loss in https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/evaluation
    def loss_function(y, y_pred_rates, y_prev):
        # Remove the initial days of train where there isn't 16 complete days of "sales".
        idx = ~torch.isnan(y_prev).any((1, 2))

        # Dims
        # B = Batch size
        # L = Sequence lenght
        # y_pred_rates - (L, B, 16, 16)
        # y_prev - (L, B, 16)
        # y_pred - (L, B, 16)

        # Dot product. Each of the 16 predictions will "attend" to the past 16 days of
        # sales.
        y_pred = (
            y_prev[idx].unsqueeze(2)
            * y_pred_rates[idx].view(
                idx.sum(), y_pred_rates.shape[1], FORECAST_WINDOW, FORECAST_WINDOW
            )
        ).sum(dim=2)

        # For convenience, the sqrt and avg operations are concluded outside of this
        # function.
        return (((y_pred + 1).log() - (y[idx] + 1).log()) ** 2).sum()

    def eval_model(model: nn.Module, seqs, splits, split="val", idx=0):
        print("Evaluating model...", end="\r")
        model.eval()
        h = torch.zeros(NUM_LAYERS, len(seqs), HIDDEN_SIZE)
        c = torch.zeros(NUM_LAYERS, len(seqs), HIDDEN_SIZE)

        # Select the set to predict and predict everything using a single Batch.
        set_sz = (splits == split).sum()
        if split == "val":
            # Get the idx-th iteration of the validation sequence plus all the train sequence.
            eval_idx = (splits == "val") | (splits == "train")

        elif split == "test":
            # Get the idx-th iteration of the test sequence plus all the train and val sequences.
            eval_idx = (splits == "val") | (splits == "train") | (splits == "test")
        x = torch.cat(
            [seq[1].unsqueeze(1)[eval_idx][: -set_sz + idx] for seq in seqs], dim=1
        )
        y_prev = torch.cat(
            [seq[2].unsqueeze(1)[eval_idx][: -set_sz + idx] for seq in seqs], dim=1
        )
        y = torch.cat(
            [seq[3].unsqueeze(1)[eval_idx][: -set_sz + idx] for seq in seqs], dim=1
        )
        ids = torch.cat(
            [seq[0].unsqueeze(0)[:, -set_sz + idx :] for seq in seqs],
            dim=0,
        )
        with torch.no_grad():
            # Whole sequences go through the model.
            y_pred_rates, _, _ = model(x, h, c)

            # But we only care for the forecast prediction of the last element of the sequence.
            y_pred_rates = y_pred_rates[-1].unsqueeze(0)
            y_prev = y_prev[-1].unsqueeze(0)
            y = y[-1].unsqueeze(0)

            loss = loss_function(y, y_pred_rates, y_prev)
            loss = torch.sqrt(loss / (x.shape[1] * FORECAST_WINDOW))

            y_preds = (
                y_prev.unsqueeze(2)
                * y_pred_rates.view(
                    -1, y_pred_rates.shape[1], FORECAST_WINDOW, FORECAST_WINDOW
                )
            ).sum(dim=2)

        print(f"Eval for split {split} - Loss {loss.item()}", end="\r")

        return loss.item(), y_preds, y, ids

    ### MAIN TRAINING LOOP
    train_lossi = []
    val_lossi = []
    h = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)
    c = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)

    for epoch in range(30):
        random.shuffle(seqs)
        train_split = splits == "train"

        # Create batches.
        batches = [
            (
                torch.cat(
                    [
                        seq[1][train_split].unsqueeze(1)
                        for seq in seqs[i : i + BATCH_SIZE]
                    ],
                    dim=1,
                ),
                torch.cat(
                    [
                        seq[2][train_split].unsqueeze(1)
                        for seq in seqs[i : i + BATCH_SIZE]
                    ],
                    dim=1,
                ),
                torch.cat(
                    [
                        seq[3][train_split].unsqueeze(1)
                        for seq in seqs[i : i + BATCH_SIZE]
                    ],
                    dim=1,
                ),
            )
            for i in range(0, len(seqs), BATCH_SIZE)
        ]

        # Reduce the learning rate at every epoch to prevent the loss to diverge.
        for pg in optimizer.param_groups:
            pg["lr"] *= 0.95

        model.train()
        epoch_losses = torch.zeros(len(batches))
        for i, (x, y_prev, y) in enumerate(batches):
            if y.sum() == 0 or x.shape[1] != BATCH_SIZE:
                continue
            optimizer.zero_grad()
            y_pred_rates, _, _ = model(x, h, c)
            loss = loss_function(y, y_pred_rates, y_prev)

            # Average the loss and take the square root (check https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/evaluation)
            loss = torch.sqrt(loss / (x.shape[0] * FORECAST_WINDOW * BATCH_SIZE))
            if (loss.item() > 1e6) or torch.isnan(loss):
                break
            print(f"Batch {i}/{len(batches)} - Loss {loss.item()}", end="\r")
            loss.backward()
            optimizer.step()
            epoch_losses[i] = loss.item()
        train_loss = epoch_losses.mean().item()
        train_lossi.append(train_loss)
        val_loss, _, _, _ = eval_model(model, seqs, splits, split="val", idx=0)
        val_lossi.append(val_loss)
        print(
            f"Epoch {epoch} - Train Loss {round(train_loss, 5)} - Val Loss {round(val_loss, 5)}"
        )

    torch.save(model, PATH)


if __name__ == "__main__":
    main()
