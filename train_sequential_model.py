import pickle
import random

import torch
import torch.nn as nn
import torch.optim as optim

########################################################################################
# DESCRIPTION
########################################################################################
# Model Input
# -----------
# The model inputs are calculated numerical and categorical features and the current
# "sales" value. Because the model is a recurrent nn, the network will somehow keep
# track of past values and will be able to identify repeating patterns, mainly short
# period patterns like weekly and monthly patterns.
# Apart from the "sales" itself, numerical features include, the week day (for weekly
# patterns), the oil prices and the distances (in nr of days)
# to the next holiday and to the next payday. The latter 2 alone should majorly
# influence the the monthly patterns and be better predictors of the sales when compared
# with just using the day of the month.
# The categorical features include the store nbr, the product being sold
# (family), the store's cluster, city, state, type, the month.


# Model Architecture
# ------------------
# The model (ForecastModel) uses an LSTM with configurable nr of layers and an output
# MLP which transforms the HIDDEN_SIZE output of the LSTM into a FORECASTING_WINDOW by 1
# output (16, 1). Categorical features are processed into the model using Embeddings of
# configurable dimension.
# The model parameters where hand tunned - increased until decent performance was
# obtained. The optimizer chosen (Adam) is typically the one obtaining the best results.
# For the learning rate, there was a simple tunning involving training for one epoch and
# registring the values of the loss obtained for each batch. By analysing a plot of the
# lr exponent vs the loss, a conclusion that around -2 exponent the losses has lower
# values.

# Sequential Model
# ----------------
# The sequential model predicts using the previous FORECASTING_WINDOW (FW = 16) values
# of the "sales" (y_prev). It outputs (1, FW) values which are combined with y_prev in
# a dot product, to generate a single output with dim (1,), for the next "sales" value
# in the sequence. This method was found to give better results than simply trying to
# predict the sales value directly or using the same method of "attending" but just to
# one specific past "sales" value (last day or same day of previous week). This
# combination of multiple previous values works similarly to attention mechanisms in
# transformers.
# For the sequential model to generate the full FW of "sales" predictions, we'll need
# to make multiple passes through the model, propagating forward the updated
# hidden (h) and cell (c) states which are update on each iteration.
# Additionally, the forecast to work, we need to update the input (x) and the y_prev
# with the values forecasted in the previous step. This method for generating
# predictions is different from how it was trained which introduces an error
# propagation through the predictions which can hinder performance for large FW.

########################################################################################

# Load inputs (generated by running data_pre_processing.py)
with open("seqs.pkl", "rb") as f:
    (
        seqs,
        splits,
        FORECAST_WINDOW,
        COLS,
        STORE_TO_I,
        I_TO_STORE,
        FAM_TO_I,
        I_TO_FAM,
        PROMO_TO_I,
        I_TO_PROMO,
        MONTH_TO_I,
        I_TO_MONTH,
        CITY_TO_I,
        I_TO_CITY,
        STATE_TO_I,
        I_TO_STATE,
        TYPE_TO_I,
        I_TO_TYPE,
        CLUSTER_TO_I,
        I_TO_CLUSTER,
    ) = pickle.load(f)


# Define model architecture.
class ForecastModel(nn.Module):
    def __init__(self, n_hidden, n_output, emb_sz=2, num_layers=2, batch_first=True):
        super(ForecastModel, self).__init__()
        self.embed_store = nn.Embedding(len(STORE_TO_I), emb_sz)
        self.embed_family = nn.Embedding(len(FAM_TO_I), emb_sz)
        self.embed_city = nn.Embedding(len(CITY_TO_I), emb_sz)
        self.embed_state = nn.Embedding(len(STATE_TO_I), emb_sz)
        self.embed_type = nn.Embedding(len(TYPE_TO_I), emb_sz)
        self.embed_cluster = nn.Embedding(len(CLUSTER_TO_I), emb_sz)
        self.embed_promo = nn.Embedding(len(PROMO_TO_I), emb_sz)
        self.embed_month = nn.Embedding(len(MONTH_TO_I), emb_sz)
        self.rnn = nn.LSTM(
            8 * emb_sz + 5, n_hidden, num_layers=num_layers, batch_first=batch_first
        )
        self.relu = nn.ReLU()
        self.out1 = nn.Linear(n_hidden, round(n_hidden / 2))
        self.out2 = nn.Linear(round(n_hidden / 2), n_output)

    def get_col(self, x, col):
        if x.ndim == 1:
            return x[COLS.index(col)].long()
        elif x.ndim == 2:
            return x[:, COLS.index(col)].long()
        elif x.ndim == 3:
            return x[:, :, COLS.index(col)].long()

    def get_cols(self, x, cols):
        if x.ndim == 1:
            return x[[COLS.index(c) for c in cols]]
        elif x.ndim == 2:
            return x[:, [COLS.index(c) for c in cols]]
        elif x.ndim == 3:
            return x[:, :, [COLS.index(c) for c in cols]]

    def forward(self, x, h, c):
        store = self.embed_store(self.get_col(x, "store"))
        family = self.embed_family(self.get_col(x, "family"))
        city = self.embed_city(self.get_col(x, "city"))
        state = self.embed_state(self.get_col(x, "state"))
        xtype = self.embed_type(self.get_col(x, "type"))
        cluster = self.embed_cluster(self.get_col(x, "cluster"))
        promo = self.embed_promo(self.get_col(x, "is_promotion"))
        month = self.embed_month(self.get_col(x, "month"))
        x_num = self.get_cols(
            x,
            [
                "sales",
                "dist_to_payday",
                "dist_to_next_holiday",
                "weekday",
                "oil_price",
            ],
        )
        x_w_embs = torch.cat(
            [store, family, city, state, xtype, cluster, promo, month, x_num],
            dim=x.ndim - 1,
        ).float()
        out_h, (h, c) = self.rnn(x_w_embs, (h, c))
        out1 = self.out1(self.relu(out_h))
        out = self.relu(self.out2(self.relu(out1)))
        return out, h, c


def main():
    # Model parameters
    HIDDEN_SIZE = 200
    NUM_LAYERS = 3
    EMB_SZ = 6
    BATCH_SIZE = 16
    PATH = f"model_sequential_{HIDDEN_SIZE}_{NUM_LAYERS}_{EMB_SZ}_{BATCH_SIZE}.pth"

    # Initialize model
    model = ForecastModel(
        HIDDEN_SIZE,
        FORECAST_WINDOW,
        emb_sz=EMB_SZ,
        num_layers=NUM_LAYERS,
        batch_first=False,
    )
    model.float()

    # Set optimizer.
    optimizer = optim.Adam(model.parameters(), lr=10**-2.2)

    # Output is the attention weights for the previous 16 days of sales.
    # Check the formula of the loss in https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/evaluation
    def loss_function(y, y_pred_rates, y_prev):
        # Remove the initial days of train where there isn't 16 complete days of "sales".
        idx = ~torch.isnan(y_prev).any((1, 2))

        # Dims
        # B = Batch size
        # L = Sequence lenght
        # y_pred_rates - (L, B, 16)
        # y_prev - (L, B, 16)
        # y_pred - (L, B, 1)

        # Dot product.
        y_pred = (y_prev[idx] * y_pred_rates[idx]).sum(dim=2)

        # For convenience, the sqrt and avg operations are concluded outside of this
        # function.
        return (((y_pred + 1).log() - (y[idx] + 1).log()) ** 2).sum()

    def eval_model(model: nn.Module, seqs, splits, split="val"):
        print("Evaluating model...", end="\r")
        model.eval()
        h_in = torch.zeros(NUM_LAYERS, len(seqs), HIDDEN_SIZE)
        c_in = torch.zeros(NUM_LAYERS, len(seqs), HIDDEN_SIZE)

        # Select the set to predict and predict everything using a single Batch.
        set_sz = (splits == split).sum()
        if split == "val":
            # Get the idx-th iteration of the validation sequence plus all the train sequence.
            eval_idx = (splits == "val") | (splits == "train")

        elif split == "test":
            # Get the idx-th iteration of the test sequence plus all the train and val sequences.
            eval_idx = (splits == "val") | (splits == "train") | (splits == "test")
        x_in = torch.cat(
            [seq[1].unsqueeze(1)[eval_idx][: -set_sz - 1] for seq in seqs], dim=1
        )
        y_prev = torch.cat(
            [seq[2].unsqueeze(1)[eval_idx][-set_sz - 1] for seq in seqs], dim=0
        )
        y = torch.cat(
            [seq[3].unsqueeze(1)[eval_idx][-set_sz - 1] for seq in seqs], dim=0
        )
        ids = torch.cat(
            [seq[0].unsqueeze(0)[:, -set_sz:] for seq in seqs],
            dim=0,
        )
        y_preds_l = []
        with torch.no_grad():
            for i in range(set_sz):
                if x_in.ndim == 2:
                    x_in = x_in.unsqueeze(0)
                y_pred_rates, h_out, c_out = model(x_in, h_in, c_in)
                y_pred_i = (y_prev * y_pred_rates[-1]).sum(dim=1)
                x_in = torch.cat(
                    [seq[1].unsqueeze(1)[eval_idx][-set_sz + i] for seq in seqs], dim=0
                )
                x_in[:, COLS.index("sales")] = y_pred_i
                y_prev = torch.cat([y_prev[:, :-1], y_pred_i.unsqueeze(1)], dim=1)
                h_in = h_out
                c_in = c_out
                y_preds_l.append(y_pred_i)

            y_preds = torch.cat([yp.unsqueeze(1) for yp in y_preds_l], dim=1)
            loss = torch.sqrt((((y_preds + 1).log() - (y + 1).log()) ** 2).mean())

        print(f"Eval for split {split} - Loss {loss.item()}", end="\r")

        return loss.item(), y_preds, y, ids

    ### MAIN TRAINING LOOP
    train_lossi = []
    val_lossi = []

    for epoch in range(30):
        random.shuffle(seqs)
        train_split = splits == "train"

        # Create batches.
        batches = [
            (
                torch.cat(
                    [
                        seq[1][train_split].unsqueeze(1)
                        for seq in seqs[i : i + BATCH_SIZE]
                    ],
                    dim=1,
                ),
                torch.cat(
                    [
                        seq[2][train_split].unsqueeze(1)
                        for seq in seqs[i : i + BATCH_SIZE]
                    ],
                    dim=1,
                ),
                torch.cat(
                    [
                        seq[3][train_split].unsqueeze(1)
                        for seq in seqs[i : i + BATCH_SIZE]
                    ],
                    dim=1,
                ),
            )
            for i in range(0, len(seqs), BATCH_SIZE)
        ]

        # Reduce the learning rate to prevent the loss to diverge.
        if epoch % 2 == 0 and epoch > 0:
            for pg in optimizer.param_groups:
                pg["lr"] *= 0.97

        model.train()
        epoch_losses = torch.zeros(len(batches))
        for i, (x, y_prev, y) in enumerate(batches):
            if y[:, 0].sum() == 0 or x.shape[1] != BATCH_SIZE:
                continue
            h = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)
            c = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)
            optimizer.zero_grad()
            loss = 0
            y_pred, _, _ = model(x, h, c)
            loss += loss_function(y[:, :, 0], y_pred, y_prev)

            # Average the loss and take the square root (check https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/evaluation)
            loss = torch.sqrt(loss / (x.shape[0] * BATCH_SIZE))
            if (loss.item() > 1e6) or torch.isnan(loss):
                break
            print(f"Batch {i}/{len(batches)} - Loss {loss.item()}", end="\r")
            loss.backward()
            optimizer.step()
            epoch_losses[i] = loss
        train_loss = epoch_losses.mean().item()
        train_lossi.append(train_loss)
        val_loss, _, _, _ = eval_model(model, seqs, splits, split="val")
        val_lossi.append(val_loss)
        print(
            f"Epoch {epoch} - Train Loss {round(train_loss, 5)} - Val Loss {round(val_loss, 5)}"
        )

    torch.save(model, PATH)


if __name__ == "__main__":
    main()
